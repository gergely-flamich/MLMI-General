{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hidden Markov Models\n",
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we implement a Kalman filter with:\n",
    "\n",
    "$$\n",
    "y_t = \\sin(\\omega t) + \\sigma^2\\epsilon_t,\\quad \\epsilon_t \\sim \\mathcal{N}(0, 1)\n",
    "$$\n",
    "\n",
    "Recall, that a Kalman filtering is just using an latent AR(1) model that has the following components:\n",
    "\n",
    " - Initial density: $ p( \\mathbf{x}_1 \\mid \\theta ) = \\mathcal{N}(\\mathbf{x}_1; \\mu_1, \\Sigma_1) $\n",
    " - Transition density: $ p( \\mathbf{x}_n \\mid \\mathbf{x}_{n - 1}, \\theta ) = \\mathcal{N}(\\mathbf{x}_n; A\\mathbf{x}_{n-1}, Q) $\n",
    " - Emission density: $ p( \\mathbf{y}_n \\mid \\mathbf{x}_{n}) = \\mathcal{N}(\\mathbf{y}_n; C\\mathbf{x}_{n}, R) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define y_t\n",
    "def y(t, omega, var):\n",
    "    noise = np.random.normal(0, var)\n",
    "    return np.sin(omega * t) + noise\n",
    "\n",
    "# Generate some data\n",
    "data = [y(t, 0.2, 0.3) for t in range(100)]\n",
    "\n",
    "plt.plot(data, marker='.', linestyle='none');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrete HMMs\n",
    "\n",
    "Assume we have observations $x_{1:T} = \\{x_1, x_2, ..., x_T \\}$.\n",
    "For discrete HMMs, the fundamental assumptions are:\n",
    "- Time is discrete (i.e. there are discrete states $s_1, s_2, ...$) and at each timestep $t$ a hidden state $s_t$ generated the observation $x_t$.\n",
    "- There are finite number of states $N$ (represented by integers 1 to $N$)\n",
    "- There are a finitie number of possible emissions $K$ (represented by integers 1 to $K$)\n",
    "- Factorisation assumption:\n",
    "$$\n",
    "    p(x_{1:T}, z_{1:T}) = p(z_1)p(x_1 \\mid z_1) \\prod_{t=1}^T p(z_t \\mid z_{t-1})p(x_t \\mid z_t)\n",
    "$$\n",
    "\n",
    "The usual components of the HMM are as follows:\n",
    "- Initial probabilities: $\\pi_n = p(z_1 = n)$\n",
    "- Transition probabilities: $T_{i, j} = p(z_t = i \\mid z_{t-1} = j)$\n",
    "- Emission probabilities: $ E_{n, k} = p(x_t = k \\mid z_t = n)$\n",
    "\n",
    "With the obvious constraints:\n",
    "- $\\sum_{n = 1}^N \\pi_n = 1$\n",
    "- $\\forall i \\in \\{1,...,N\\}:\\quad \\sum_{n = 1}^N T_{i, n} = 1$\n",
    "- $\\forall n \\in \\{1,...,N\\}:\\quad \\sum_{k = 1}^K E_{n, k} = 1$ \n",
    "\n",
    "Given some data, we might wish to infer the values $\\theta = \\{\\pi, T, E\\}$.\n",
    "\n",
    "This can be done using the EM algorithm. The flavour presented below is also called the Baum-Welch algorithm.\n",
    "\n",
    "### The E-step\n",
    "\n",
    "The E-step of Baum-Welch is also referred to as the Forward-Backward algorithm, and it works as follows:\n",
    "\n",
    "__The big picture:__ We want to calculate the posterior of the latent variables for fixed $\\theta$: $p(z_t \\mid x_{1:N}, \\theta) \\propto p(z_t, x_{1:N} \\mid \\theta)$.\n",
    "\n",
    "We wish to calculate: $p(z_t, x_{1:N} \\mid \\theta)$ for every $z_t$. We can split this up as follows:\n",
    "\n",
    "$$\n",
    "p(z_t, x_{1:N} \\mid \\theta) = p(z_t, x_{1:t} \\mid \\theta)p(x_{t+1:N} \\mid z_t, x_{1:t}, \\theta) = p(z_t, x_{1:t} \\mid \\theta)\\underbrace{p(x_{t+1:N} \\mid z_t, \\theta)}_{\\text{since } x_{t+1:N} \\text{ are cond. indep. of } x_{1:t} \\text{ given } z_t}\n",
    "$$\n",
    "\n",
    "We will calculate the first term on the LHS using the forward part of the algorithm, and the second part by the backward algorithm.\n",
    "\n",
    "#### Forward Algorithm\n",
    "\n",
    "Consider first $p(z_t, x_{1:t} \\mid \\theta)$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\alpha_t(s) = p(z_t = s, x_{1:t} \\mid \\theta) \n",
    "& = p(x_t = k \\mid z_t = s, x_{1:t - 1}, \\theta)p(z_t = s\\mid x_{1: t - 1}, \\theta)\\\\ \n",
    "& = \\underbrace{p(x_t = k\\mid z_t = s, \\theta)}_{\\text{since } x_{t} \\text{ is cond. indep. of } x_{1:t-1} \\text{ given } z_t}p(z_t = s\\mid x_{1: t - 1}, \\theta)\\\\ \n",
    "& = p(x_t = k\\mid z_t = s, \\theta) \\sum_{n = 1}^N p(z_t = s, z_{t - 1} = n \\mid x_{1:t-1}, \\theta)\\\\ \n",
    "& = \\underbrace{p(x_t = k\\mid z_t = s, \\theta)}_{E_{s, k}} \\sum_{n = 1}^N \\underbrace{p(z_t = s \\mid  z_{t - 1} = n, \\theta)}_{T_{s, n}} \\underbrace{p(z_{t - 1} = n \\mid x_{1:t-1}, \\theta)}_{\\alpha_{t - 1}(n)}\\\\ \n",
    "& = E_{s, k} \\sum_{n = 1}^N T_{s, n}\\alpha_{t - 1}(n)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Hence we have obtained the recursive relationship:\n",
    "\n",
    "$$\n",
    "\\alpha_t(s) = E_{s, k} \\sum_{n = 1}^N T_{s, n}\\alpha_{t - 1}(n)\n",
    "$$\n",
    "\n",
    "for $t \\geq 2$. For $t = 1$, we have:\n",
    "$$\n",
    "\\alpha_1(s) = p(z_1 = s, x_1 = k) = p(x_1 = k \\mid z_1 = s)p(z_1 = s) = E_{s, k}\\pi_s\n",
    "$$\n",
    "\n",
    "In vectorised form, let $\\alpha_t = [\\alpha_t(1), \\alpha_t(2), ..., \\alpha_t(N)]^T$. Then, we have:\n",
    "\n",
    "$$\n",
    "\\alpha_t = \\overbrace{E_{\\cdot, k}}^{N \\times 1} \\odot \\overbrace{ \\underbrace{T}_{N \\times N} \\underbrace{\\alpha_{t - 1}^T}_{N\\times 1}}^{N \\times 1}\n",
    "$$\n",
    "\n",
    "where $\\odot$ is elementwise multiplication.\n",
    "\n",
    "#### Backward Algorithm\n",
    "\n",
    "Now, consider $p(x_{t+1:T} \\mid z_t, \\theta)$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\beta_t(s) = p(x_{t+1:T} \\mid z_t = s, \\theta) & = \\sum_{n = 1}^N p(x_{t+1:T}, z_{t + 1} = n\\mid z_t = s, \\theta) \\\\\n",
    "& = \\sum_{n = 1}^N p(x_{t+2:T}\\mid z_{t + 1} = n, x_{t + 1} = k, z_t = s, \\theta)p(x_{t + 1} = k \\mid z_{t + 1} = n, z_t = s, \\theta)p(z_{t + 1} = n \\mid z_t = s, \\theta) \\\\\n",
    "& = \\sum_{n = 1}^N \\underbrace{p(x_{t+2:T}\\mid z_{t + 1} = n, \\theta)}_{\\text{cond. indep.}}\\underbrace{p(x_{t + 1} = k \\mid z_{t + 1} = n, \\theta)}_{\\text{cond. indep.}}p(z_{t + 1} = n \\mid z_t = s, \\theta) \\\\\n",
    "& = \\sum_{n = 1}^N \\beta_{t + 1}(n)E_{n, k}T_{n, s}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Hence we have obtained the recursive relationship:\n",
    "\n",
    "$$\n",
    "\\beta(s) = \\sum_{n = 1}^N \\beta_{t + 1}(n)E_{n, k}T_{n, s}\n",
    "$$\n",
    "\n",
    "for $t \\leq T - 1$. For $t = T$ we have:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\beta_{T - 1}(s) & = \\sum_{n = 1}^N p(x_T, z_T = n\\mid z_{T - 1} = s, \\theta) \\\\\n",
    "& = \\sum_{n = 1}^N p(x_T \\mid z_T = n, \\theta)p(z_T = n \\mid z_{T - 1} = s, \\theta) \\\\\n",
    "& = \\sum_{n = 1}^N E_{n, k}T_{n, s} = \\sum_{n = 1}^N \\beta_{T}(n)E_{n, k}T_{n, s}.\n",
    "\\end{align*}\n",
    "$$\n",
    "For the last equality to hold, we must have $\\beta_T(s) = 1$ for every $s$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data\n",
    "xs = \"AASDFAGASDGSADGDSGFAGAFSDGASGFASGDFAGDFGASDGASFDGASFDGASFDGFASGDFASGDFAGSFDGASDGFASFSGAGAFSGDGFAGFSGFDFDFDGAGSFDFDFAGAGSFDGF\"\n",
    "\n",
    "# Define the parameters of the HMM\n",
    "num_states = 3\n",
    "num_symbols = 25\n",
    "\n",
    "# Dummy names for later\n",
    "N = num_states\n",
    "K = num_symbols\n",
    "L = len(xs)\n",
    "\n",
    "# Components of the HMM\n",
    "pi = np.random.rand(N,1).ravel()\n",
    "T = np.random.rand(N, N)\n",
    "E = np.random.rand(N, K)\n",
    "\n",
    "# Enforce the 3 probability constraints\n",
    "pi = pi/np.sum(pi)\n",
    "               \n",
    "T_norm = np.sum(T, axis=1)\n",
    "for n in range(N):\n",
    "    T[n, :] = T[n, :]/T_norm[n]\n",
    "\n",
    "E_norm = np.sum(E, axis=0)\n",
    "for k in range(K):\n",
    "    E[:, k] = E[:, k]/E_norm[k]\n",
    "    \n",
    "# Transform the input data to states\n",
    "xs_s = list(map(lambda x: ord(x) - ord('A'), xs))     \n",
    "\n",
    "log_alphas = np.ndarray((L, N))\n",
    "log_betas = np.ndarray((L, N))\n",
    "\n",
    "log_pi = np.log(pi)\n",
    "log_T = np.log(T)\n",
    "log_E = np.log(E)\n",
    "\n",
    "# Set the base cases for alphas and betas\n",
    "log_alphas[0, :] = np.log(E[:, xs_s[0]]) + log_pi\n",
    "log_betas[0, :] = np.zeros((1, N))\n",
    "\n",
    "# Function for the log-sum-exp trick\n",
    "# log(alpha_t(n)) = log(E_{n, x_t}) + b + log(sum_{n = 1}^N exp(a_n - b))\n",
    "def log_sum_exp(terms):\n",
    "    if len(terms) == 0: return 0\n",
    "    \n",
    "    b = np.max(terms)\n",
    "    ts = terms - b\n",
    "    \n",
    "    return b + np.log(np.sum(np.exp(ts)))\n",
    "\n",
    "# Calculate the rest of the alphas based on the formula\n",
    "for t in range(1, L):\n",
    "    for n in range(N):\n",
    "        a = log_T[n, :] + log_alphas[t - 1, :]\n",
    "        \n",
    "        log_alphas[t, n] = log_E[n, xs_s[t]] + log_sum_exp(a)\n",
    "\n",
    "# Calculate the betas\n",
    "for t in range(1, L):\n",
    "    for n in range(N):\n",
    "        a = log_E[:, xs_s[t]] + log_T[:, n] + log_betas[t - 1, :]\n",
    "\n",
    "        log_betas[t, n] = log_sum_exp(a)\n",
    "        \n",
    "# We calculated stuff backwards, so flip along time axis\n",
    "log_betas = np.flip(log_betas, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The M-step\n",
    "\n",
    "$\\newcommand{\\diff}[2]{\\frac{\\partial #1}{\\partial #2}}$\n",
    "\n",
    "__The big picture:__ Given the posterior, we want to calculate the maximum likelihood estimate for $\\theta$.\n",
    "\n",
    "We want to maximise the log-likelihood, however, it is easier to maximise the expected value of the joint w.r.t. the posterior $q(z_{1:T}) = p(z_{1:T} \\mid x_{1:T}, \\theta)$, which can be show to be equivalent:\n",
    "$$\n",
    "\\begin{align*}\n",
    "L'(\\theta) &= \\mathbb{E}_{q}[\\log p(z_{1:T}, x_{1:T})] \\\\\n",
    "&= \\mathbb{E}_{q}\\left[ \\log \\left\\{ p(z_1)p(x_1 \\mid z_1) \\prod_{t=2}^T p(z_t\\mid z_{t-1})p(x_t\\mid z_t)\\right\\} \\right ]\\quad\\text{by the Factorisation Assumption} \\\\\n",
    "&=\\mathbb{E}_{q}\\left[ \\log p(z_1) + \\log \\left\\{ \\prod_{t=1}^T p(x_t \\mid z_t)\\right\\} + \\log\\left\\{\\prod_{t=2}^T p(z_t\\mid z_{t-1})\\right\\} \\right ] \\\\\n",
    "&=\\mathbb{E}_{q}\\left[ \\log p(z_1) + \\sum_{t=1}^T \\log p(x_t \\mid z_t) + \\sum_{t=2}^T\\log p(z_t\\mid z_{t-1})\\right ] \\\\\n",
    "&=\\mathbb{E}_{q(z_1)}\\left[ \\log p(z_1) \\right] + \\sum_{t=1}^T \\mathbb{E}_{q(z_t)}\\left[\\log p(x_t \\mid z_t)\\right] + \\sum_{t=2}^T \\mathbb{E}_{q(z_t, z_{t-1})}\\left[\\log p(z_t\\mid z_{t-1})\\right ] \\quad\\text{by the linearity of } \\mathbb{E}[\\cdot]\\\\\n",
    "&=\\sum_{n=1}^N q(z_1 = n)\\log p(z_1 = n) + \\sum_{t=1}^T \\sum_{n=1}^N q(z_t=n)\\log p(x_t \\mid z_t = n) \\\\ & \\quad\\quad + \\sum_{t=2}^T \\sum_{n=1}^N\\sum_{m=1}^M q(z_t = n \\mid z_{t-1} = m) \\log p(z_t = n\\mid z_{t-1} = m) \\\\\n",
    "&=\\sum_{n=1}^N q(z_1 = n)\\log\\pi_n + \\sum_{t=1}^T \\sum_{n=1}^N q(z_t=n)\\log E_{n,x_t} + \\sum_{t=2}^T \\sum_{n=1}^N\\sum_{m=1}^N q(z_t = n , z_{t-1} = m) \\log T_{n, m} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "However, we cannot maximise $L'(\\theta)$ just yet, as it does not incorporate the 3 probability assumptions. We therefore define a new function $L(\\theta)$ which does incorporate them using Lagrange multipliers:\n",
    "\n",
    "$$\n",
    "L(\\theta) = L'(\\theta) + \\alpha \\underbrace{\\left [ \\sum_{n = 1}^N \\pi_n - 1 \\right]}_{\\pi \\text{ is a pmf}} + \\beta \\underbrace{\\left [ \\sum_{n=1}^N \\sum_{k=1}^K  E_{n,k} - N \\right ]}_{\\text{each row $E$ of is a pmf}} + \\gamma \\underbrace{\\left [ \\sum_{n=1}^N \\sum_{m=1}^N  T_{n,m} - N \\right ]}_{\\text{each column $T$ of is a pmf}}\n",
    "$$\n",
    "\n",
    "This can now be differentiated w.r.t. $\\pi, E$ and $T$. \n",
    "\n",
    "First we take care of $\\pi$:\n",
    "\n",
    "$$\n",
    "\\diff{L(\\theta)}{\\pi_s} = \\frac{q(z_1 = s)}{\\pi_s} + \\alpha = 0 \\quad\\Rightarrow\\quad \\pi_s = -\\frac{q(z_1 = s)}{\\alpha}\n",
    "$$\n",
    "Then, since $\\sum_{n = 1}^N \\pi_n = 1$, we have\n",
    "\n",
    "$$\n",
    "\\sum_{n = 1}^N \\pi_s = -\\frac{1}{\\alpha}\\sum_{n = 1}^N q(z_1 = n) = 1 \\quad\\Rightarrow\\quad -\\sum_{n = 1}^N q(z_1 = n) = \\alpha\n",
    "$$\n",
    "\n",
    "Substituting this back, we get:\n",
    "$$\n",
    "    \\pi_s = \\frac{q(z_1 = s)}{\\sum_{n = 1}^N q(z_1 = n)}\n",
    "$$\n",
    "\n",
    "Next, we take care of $E$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\diff{L(\\theta)}{E_{n', k'}} &= \\beta + \\diff{}{E_{n', k'}}\\sum_{t=1}^T \\sum_{n=1}^N q(z_t=n)\\log E_{n,x_t}\\\\\n",
    "& = \\beta + \\sum_{t=1}^T \\delta(x_t = k') \\frac{q(z_t=n')}{E_{n',k'}} = 0 \\quad \\text{where $\\delta$ is the Kronecker delta}\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "Whence\n",
    "$$\n",
    "E_{n', k'} = -\\frac{1}{\\beta}\\sum_{t=1}^T \\delta(x_t = k') q(z_t=n')\n",
    "$$\n",
    "\n",
    "Now, using the constraint that $\\forall n: \\sum_{k=1}^K  E_{n,k} = 1 $:\n",
    "\n",
    "$$\n",
    "     \\sum_{k=1}^K  -\\frac{1}{\\beta}\\sum_{t=1}^T \\delta(x_t = k) q(z_t=n) = N \\quad \\Rightarrow \\quad \\beta = -\\sum_{t=1}^T \\underbrace{\\sum_{k=1}^K\\delta(x_t = k)}_{x_t = k \\text{ is true for one and only one } k} q(z_t=n) = -\\sum_{t=1}^T q(z_t=n) \n",
    "$$\n",
    "\n",
    "Therefore, we have:\n",
    "$$\n",
    "    E_{n, k} = \\frac{\\sum_{t=1}^T \\delta(x_t = k) q(z_t=n)}{\\sum_{t=1}^T q(z_t=n)} \\\\\n",
    "$$\n",
    "\n",
    "Finally, we take care of $T$:\n",
    "\n",
    "$$\n",
    "    \\diff{L(\\theta)}{T_{n', m'}} = \\gamma + \\sum_{t=2}^T \\frac{q(z_t = n', z_{t-1} = m')}{T_{n', m'}} = 0 \\quad \\Rightarrow\\quad T_{n', m'} = - \\frac{1}{\\gamma}\\sum_{t=2}^T q(z_t = n', z_{t-1} = m')\n",
    "$$\n",
    "\n",
    "Using the constraint that $\\forall m: \\sum_{n=1}^N T_{n,m} = 1$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "&\\sum_{n=1}^N - \\frac{1}{\\gamma}\\sum_{t=2}^T q(z_t = n, z_{t-1} = m) = N \\\\ \\\\\n",
    "\\Rightarrow\\quad \n",
    "\\gamma &= - \\sum_{n=1}^N \\sum_{t=2}^T q(z_t = n, z_{t-1} = m) \\\\\n",
    " &= - \\sum_{t=2}^T \\sum_{n=1}^N q(z_t = n\\mid z_{t-1} = m)q(z_{t-1} = m) \\quad \\text{reorder summation}\\\\\n",
    "&= - \\sum_{t=2}^T \\sum_{n=1}^N  T_{n, m}q(z_{t-1} = m) \\\\\n",
    "&= - \\sum_{t=2}^T q(z_{t-1} = m)\\underbrace{\\sum_{n=1}^N T_{n, m}}_{\\text{Every column sums to 1}} \\\\\n",
    "&= - \\sum_{t=2}^T q(z_{t-1} = m)= - \\sum_{t=1}^{T-1} q(z_{t} = m) \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "And hence:\n",
    "\n",
    "$$\n",
    "T_{n, m} = \\frac{\\sum_{t=2}^T q(z_t = n, z_{t-1} = m)}{\\sum_{t=1}^{T-1} q(z_{t} = m)}\n",
    "$$\n",
    "\n",
    "Finally, we note that we have\n",
    "$$\n",
    "q(z_t = n) = p(z_t = n \\mid x_{1:T}, \\theta) = \\frac{p(z_t = n, x_{1:T} \\mid \\theta)}{p(x_{1:T})} = \\frac{\\alpha_t(n)\\beta_t(n)}{\\sum_{m=1}^N\\alpha_t(m)\\beta_t(m)}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\begin{align*}\n",
    "q(z_t = n, z_{t-1} = m) &= p(z_t = n, z_{t-1} = m \\mid x_{1:T}, \\theta) \\\\\n",
    "&= \\frac{p(z_t = n, z_{t-1} = m, x_{1:T}\\mid \\theta)}{p(x_{1:T})} \\\\\n",
    "&= \\frac{p(z_t = n, z_{t-1} = m, x_{1:t - 1}, x_t, x_{t+1:T}\\mid \\theta)}{p(x_{1:T})} \\\\\n",
    "&= \\frac{p(x_t \\mid z_t = n, \\theta)p(z_t = n, z_{t-1} = m, x_{1:t - 1}, x_{t+1:T}\\mid \\theta)}{p(x_{1:T})} \\\\\n",
    "&= \\frac{E_{n, x_t}p(x_{t+1:T}\\mid z_t = n, \\theta)p(z_t = n, z_{t-1} = m, x_{1:t - 1}\\mid \\theta)}{p(x_{1:T})} \\\\\n",
    "&= \\frac{E_{n, x_t}\\beta_t(n)p(z_t = n \\mid z_{t-1} = m, \\theta)p(z_{t-1} = m, x_{1:t - 1}\\mid \\theta)}{p(x_{1:T})} \\\\\n",
    "&= \\frac{E_{n, x_t}\\beta_t(n)T_{n, m}\\alpha_{t-1}(m)}{p(x_{1:T})} \\\\\n",
    "&= \\frac{E_{n, x_t}\\beta_t(n)T_{n, m}\\alpha_{t-1}(m)}{\\sum_{n'=1}^N\\sum_{m'=1}^N E_{n', x_t}\\beta_t(n')T_{n', m'}\\alpha_{t-1}(m')} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "### Summary\n",
    "\n",
    "We need to implement the following update rules:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\pi_s &= \\frac{q(z_1 = s)}{\\sum_{n = 1}^N q(z_1 = n)} \\\\ \\\\\n",
    "    E_{n, k} &= \\frac{\\sum_{t=1}^T \\delta(x_t = k) q(z_t=n)}{\\sum_{t=1}^T q(z_t=n)} \\\\ \\\\\n",
    "    T_{n, m} &= \\frac{\\sum_{t=2}^T q(z_t = n, z_{t-1} = m)}{\\sum_{t=1}^{T-1} q(z_{t} = m)}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Where $q$ is calculated as:\n",
    "$$\n",
    "\\begin{align*}\n",
    "q(z_t = n) &= \\frac{\\alpha_t(n)\\beta_t(n)}{\\sum_{m=1}^N\\alpha_t(m)\\beta_t(m)} \\\\ \\\\\n",
    "q(z_t = n, z_{t-1} = m) &= \\frac{E_{n, x_t}\\beta_t(n)T_{n, m}\\alpha_{t-1}(m)}{\\sum_{n'=1}^N\\sum_{m'=1}^N E_{n', x_t}\\beta_t(n')T_{n', m'}\\alpha_{t-1}(m')} \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Given these three update rules, we can now perform the M-step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Qs\n",
    "\n",
    "log_uni = np.ndarray((L, N))\n",
    "    \n",
    "for t in range(L):\n",
    "    for n in range(N):\n",
    "        log_uni[t, n] = log_alphas[t, n] + log_betas[t, n]\n",
    "        \n",
    "    log_uni[t, :] -= log_sum_exp(log_uni[t, :])\n",
    "\n",
    "log_joint = np.ndarray((L-1, N, N))\n",
    "for t in range(1, L):\n",
    "    for n in range(N):\n",
    "        for m in range(N):\n",
    "            log_joint[t - 1, n, m] = log_E[n, xs_s[t]] + log_betas[t, n] + log_T[n, m] + log_alphas[t - 1, m]\n",
    "    \n",
    "    log_joint[t - 1, :, :] -= log_sum_exp(log_joint[t - 1, :, :])\n",
    "    \n",
    "# Calculate updated parameters\n",
    "log_pi_star = np.ndarray(N)\n",
    "log_E_star = np.ndarray((N, K))\n",
    "log_T_star = np.ndarray((N, N))\n",
    "\n",
    "for n in range(N):\n",
    "    log_pi_star[n] = log_uni[0, n]\n",
    "    \n",
    "log_pi_star -= log_sum_exp(log_pi_star)\n",
    "\n",
    "for k in range(K):\n",
    "    for n in range(N):\n",
    "        log_qs = log_uni[:, n]\n",
    "        \n",
    "        log_E_star[n, k] = log_sum_exp(log_qs[xs_s == k]) - log_sum_exp(log_qs) \n",
    "            \n",
    "for n in range(N):\n",
    "    for m in range(N):\n",
    "        log_T_star[n, n] = log_sum_exp(log_joint[:, n, m]) - log_sum_exp(log_uni[:-1, m])\n",
    "        \n",
    "# Perform the update\n",
    "log_pi = log_pi_star\n",
    "log_E = log_E_star\n",
    "log_T = log_T_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.00998711, 0.69403278, 0.99941776])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
