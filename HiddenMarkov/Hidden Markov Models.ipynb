{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hidden Markov Models\n",
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we implement a Kalman filter with:\n",
    "\n",
    "$$\n",
    "y_t = \\sin(\\omega t) + \\sigma^2\\epsilon_t,\\quad \\epsilon_t \\sim \\mathcal{N}(0, 1)\n",
    "$$\n",
    "\n",
    "Recall, that a Kalman filtering is just using an latent AR(1) model that has the following components:\n",
    "\n",
    " - Initial density: $ p( \\mathbf{x}_1 \\mid \\theta ) = \\mathcal{N}(\\mathbf{x}_1; \\mu_1, \\Sigma_1) $\n",
    " - Transition density: $ p( \\mathbf{x}_n \\mid \\mathbf{x}_{n - 1}, \\theta ) = \\mathcal{N}(\\mathbf{x}_n; A\\mathbf{x}_{n-1}, Q) $\n",
    " - Emission density: $ p( \\mathbf{y}_n \\mid \\mathbf{x}_{n}) = \\mathcal{N}(\\mathbf{y}_n; C\\mathbf{x}_{n}, R) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define y_t\n",
    "def y(t, omega, var):\n",
    "    noise = np.random.normal(0, var)\n",
    "    return np.sin(omega * t) + noise\n",
    "\n",
    "# Generate some data\n",
    "data = [y(t, 0.2, 0.3) for t in range(100)]\n",
    "\n",
    "plt.plot(data, marker='.', linestyle='none');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrete HMMs\n",
    "\n",
    "Assume we have observations $x_{1:T} = \\{x_1, x_2, ..., x_T \\}$.\n",
    "For discrete HMMs, the fundamental assumptions are:\n",
    "- Time is discrete (i.e. there are discrete states $s_1, s_2, ...$) and at each timestep $t$ a hidden state $s_t$ generated the observation $x_t$.\n",
    "- There are finite number of states $N$ (represented by integers 1 to $N$)\n",
    "- There are a finitie number of possible emissions $K$ (represented by integers 1 to $K$)\n",
    "- Factorisation assumption:\n",
    "$$\n",
    "    p(x_{1:T}, z_{1:T}) = p(z_1)p(x_1 \\mid z_1) \\prod_{t=1}^T p(z_t \\mid z_{t-1})p(x_t \\mid z_t)\n",
    "$$\n",
    "\n",
    "The usual components of the HMM are as follows:\n",
    "- Initial probabilities: $\\pi_n = p(z_1 = n)$\n",
    "- Transition probabilities: $T_{i, j} = p(z_t = i \\mid z_{t-1} = j)$\n",
    "- Emission probabilities: $ E_{n, k} = p(x_t = k \\mid z_t = n)$\n",
    "\n",
    "With the obvious constraints:\n",
    "- $\\sum_{n = 1}^N \\pi_n = 1$\n",
    "- $\\forall i \\in \\{1,...,N\\}:\\quad \\sum_{n = 1}^N T_{i, n} = 1$\n",
    "- $\\forall n \\in \\{1,...,N\\}:\\quad \\sum_{k = 1}^K E_{n, k} = 1$ \n",
    "\n",
    "Given some data, we might wish to infer the values $\\theta = \\{\\pi, T, E\\}$.\n",
    "\n",
    "This can be done using the EM algorithm. The flavour presented below is also called the Baum-Welch algorithm.\n",
    "\n",
    "### The E-step\n",
    "\n",
    "The E-step of Baum-Welch is also referred to as the Forward-Backward algorithm, and it works as follows:\n",
    "\n",
    "__The big picture:__ We want to calculate the posterior of the latent variables for fixed $\\theta$: $p(z_t \\mid x_{1:N}, \\theta) \\propto p(z_t, x_{1:N} \\mid \\theta)$.\n",
    "\n",
    "We wish to calculate: $p(z_t, x_{1:N} \\mid \\theta)$ for every $z_t$. We can split this up as follows:\n",
    "\n",
    "$$\n",
    "p(z_t, x_{1:N} \\mid \\theta) = p(z_t, x_{1:t} \\mid \\theta)p(x_{t+1:N} \\mid z_t, x_{1:t}, \\theta) = p(z_t, x_{1:t} \\mid \\theta)\\underbrace{p(x_{t+1:N} \\mid z_t, \\theta)}_{\\text{since } x_{t+1:N} \\text{ are cond. indep. of } x_{1:t} \\text{ given } z_t}\n",
    "$$\n",
    "\n",
    "We will calculate the first term on the LHS using the forward part of the algorithm, and the second part by the backward algorithm.\n",
    "\n",
    "#### Forward Algorithm\n",
    "\n",
    "Consider first $p(z_t, x_{1:t} \\mid \\theta)$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\alpha_t(s) = p(z_t = s, x_{1:t} \\mid \\theta) \n",
    "& = p(x_t = k \\mid z_t = s, x_{1:t - 1}, \\theta)p(z_t = s\\mid x_{1: t - 1}, \\theta)\\\\ \n",
    "& = \\underbrace{p(x_t = k\\mid z_t = s, \\theta)}_{\\text{since } x_{t} \\text{ is cond. indep. of } x_{1:t-1} \\text{ given } z_t}p(z_t = s\\mid x_{1: t - 1}, \\theta)\\\\ \n",
    "& = p(x_t = k\\mid z_t = s, \\theta) \\sum_{n = 1}^N p(z_t = s, z_{t - 1} = n \\mid x_{1:t-1}, \\theta)\\\\ \n",
    "& = \\underbrace{p(x_t = k\\mid z_t = s, \\theta)}_{E_{s, k}} \\sum_{n = 1}^N \\underbrace{p(z_t = s \\mid  z_{t - 1} = n, \\theta)}_{T_{s, n}} \\underbrace{p(z_{t - 1} = n \\mid x_{1:t-1}, \\theta)}_{\\alpha_{t - 1}(n)}\\\\ \n",
    "& = E_{s, k} \\sum_{n = 1}^N T_{s, n}\\alpha_{t - 1}(n)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Hence we have obtained the recursive relationship:\n",
    "\n",
    "$$\n",
    "\\alpha_t(s) = E_{s, k} \\sum_{n = 1}^N T_{s, n}\\alpha_{t - 1}(n)\n",
    "$$\n",
    "\n",
    "for $t \\geq 2$. For $t = 1$, we have:\n",
    "$$\n",
    "\\alpha_1(s) = p(z_1 = s, x_1 = k) = p(x_1 = k \\mid z_1 = s)p(z_1 = s) = E_{s, k}\\pi_s\n",
    "$$\n",
    "\n",
    "In vectorised form, let $\\alpha_t = [\\alpha_t(1), \\alpha_t(2), ..., \\alpha_t(N)]^T$. Then, we have:\n",
    "\n",
    "$$\n",
    "\\alpha_t = \\overbrace{E_{\\cdot, k}}^{N \\times 1} \\odot \\overbrace{ \\underbrace{T}_{N \\times N} \\underbrace{\\alpha_{t - 1}^T}_{N\\times 1}}^{N \\times 1}\n",
    "$$\n",
    "\n",
    "where $\\odot$ is elementwise multiplication.\n",
    "\n",
    "#### Backward Algorithm\n",
    "\n",
    "Now, consider $p(x_{t+1:T} \\mid z_t, \\theta)$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\beta_t(s) = p(x_{t+1:T} \\mid z_t = s, \\theta) & = \\sum_{n = 1}^N p(x_{t+1:T}, z_{t + 1} = n\\mid z_t = s, \\theta) \\\\\n",
    "& = \\sum_{n = 1}^N p(x_{t+2:T}\\mid z_{t + 1} = n, x_{t + 1} = k, z_t = s, \\theta)p(x_{t + 1} = k \\mid z_{t + 1} = n, z_t = s, \\theta)p(z_{t + 1} = n \\mid z_t = s, \\theta) \\\\\n",
    "& = \\sum_{n = 1}^N \\underbrace{p(x_{t+2:T}\\mid z_{t + 1} = n, \\theta)}_{\\text{cond. indep.}}\\underbrace{p(x_{t + 1} = k \\mid z_{t + 1} = n, \\theta)}_{\\text{cond. indep.}}p(z_{t + 1} = n \\mid z_t = s, \\theta) \\\\\n",
    "& = \\sum_{n = 1}^N \\beta_{t + 1}(n)E_{n, k}T_{n, s}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Hence we have obtained the recursive relationship:\n",
    "\n",
    "$$\n",
    "\\beta(s) = \\sum_{n = 1}^N \\beta_{t + 1}(n)E_{n, k}T_{n, s}\n",
    "$$\n",
    "\n",
    "for $t \\leq T - 1$. For $t = T$ we have:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\beta_{T - 1}(s) & = \\sum_{n = 1}^N p(x_T, z_T = n\\mid z_{T - 1} = s, \\theta) \\\\\n",
    "& = \\sum_{n = 1}^N p(x_T \\mid z_T = n, \\theta)p(z_T = n \\mid z_{T - 1} = s, \\theta) \\\\\n",
    "& = \\sum_{n = 1}^N E_{n, k}T_{n, s} = \\sum_{n = 1}^N \\beta_{T}(n)E_{n, k}T_{n, s}.\n",
    "\\end{align*}\n",
    "$$\n",
    "For the last equality to hold, we must have $\\beta_T(s) = 1$ for every $s$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data\n",
    "xs = \"AASDFAGASDGSADGDSGFAGAFSDGASGFASGDFAGDFGASDGASFDGASFDGASFDGFASGDFASGDFAGSFDGASDGFASFSGAGAFSGDGFAGFSGFDFDFDGAGSFDFDFAGAGSFDGF\"\n",
    "\n",
    "# Define the parameters of the HMM\n",
    "num_states = 3\n",
    "num_symbols = 25\n",
    "\n",
    "# Dummy names for later\n",
    "N = num_states\n",
    "K = num_symbols\n",
    "L = len(xs)\n",
    "\n",
    "# Components of the HMM\n",
    "pi = np.random.rand(N,1).ravel()\n",
    "T = np.random.rand(N, N)\n",
    "E = np.random.rand(N, K)\n",
    "\n",
    "# Enforce the 3 probability constraints\n",
    "pi = pi/np.sum(pi)\n",
    "               \n",
    "T_norm = np.sum(T, axis=1)\n",
    "for n in range(N):\n",
    "    T[n, :] = T[n, :]/T_norm[n]\n",
    "\n",
    "E_norm = np.sum(E, axis=0)\n",
    "for k in range(K):\n",
    "    E[:, k] = E[:, k]/E_norm[k]\n",
    "    \n",
    "# Transform the input data to states\n",
    "xs_s = list(map(lambda x: ord(x) - ord('A'), xs))     \n",
    "\n",
    "log_alphas = np.ndarray((L, N))\n",
    "log_betas = np.ndarray((L, N))\n",
    "\n",
    "log_T = np.log(T)\n",
    "log_E = np.log(E)\n",
    "\n",
    "# Set the base cases for alphas and betas\n",
    "log_alphas[0, :] = np.log(E[:, xs_s[0]]) + np.log(pi)\n",
    "log_betas[0, :] = np.zeros((1, N))\n",
    "\n",
    "# Calculate the rest of the alphas based on the formula\n",
    "for t in range(1, L):\n",
    "    for n in range(N):\n",
    "        a = log_T[n, :] + log_alphas[t - 1, :]\n",
    "        b = np.max(a)\n",
    "        a = a - b\n",
    "        \n",
    "        # log(alpha_t(n)) = log(E_{n, x_t}) + b + log(sum_{n = 1}^N exp(a_n - b))\n",
    "        log_alphas[t, n] = log_E[n, xs_s[t]] + b + np.log(np.sum(np.exp(a)))\n",
    "\n",
    "# Calculate the betas\n",
    "for t in range(1, L):\n",
    "    for n in range(N):\n",
    "        a = log_E[:, xs_s[t]] + log_T[:, n] + log_betas[t - 1, :]\n",
    "        b = np.max(a)\n",
    "        a = a - b\n",
    "        \n",
    "        # log(alpha_t(n)) = log(E_{n, x_t}) + b + log(sum_{n = 1}^N exp(a_n - b))\n",
    "        log_betas[t, n] = b + np.log(np.sum(np.exp(a)))\n",
    "        \n",
    "# We calculated stuff backwards, so flip along time axis\n",
    "log_betas = np.flip(log_betas, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The M-step\n",
    "\n",
    "$\\newcommand{\\diff}[2]{\\frac{\\partial #1}{\\partial #2}}$\n",
    "\n",
    "__The big picture:__ Given the posterior, we want to calculate the maximum likelihood estimate for $\\theta$.\n",
    "\n",
    "We want to maximise the log-likelihood, however, it is easier to maximise the expected value of the joint w.r.t. the posterior $q(z_{1:T}) = p(z_{1:T}, x_{1:T}\\mid \\theta)$, which can be show to be equivalent:\n",
    "$$\n",
    "\\begin{align*}\n",
    "L'(\\theta) &= \\mathbb{E}_{q}[\\log p(z_{1:T}, x_{1:T})] \\\\\n",
    "&= \\mathbb{E}_{q}\\left[ \\log \\left\\{ p(z_1)p(x_1 \\mid z_1) \\prod_{t=2}^T p(z_t\\mid z_{t-1})p(x_t\\mid z_t)\\right\\} \\right ]\\quad\\text{by the Factorisation Assumption} \\\\\n",
    "&=\\mathbb{E}_{q}\\left[ \\log p(z_1) + \\log \\left\\{ \\prod_{t=1}^T p(x_t \\mid z_t)\\right\\} + \\log\\left\\{\\prod_{t=2}^T p(z_t\\mid z_{t-1})\\right\\} \\right ] \\\\\n",
    "&=\\mathbb{E}_{q}\\left[ \\log p(z_1) + \\sum_{t=1}^T \\log p(x_t \\mid z_t) + \\sum_{t=2}^T\\log p(z_t\\mid z_{t-1})\\right ] \\\\\n",
    "&=\\mathbb{E}_{q(z_1)}\\left[ \\log p(z_1) \\right] + \\sum_{t=1}^T \\mathbb{E}_{q(z_t)}\\left[\\log p(x_t \\mid z_t)\\right] + \\sum_{t=2}^T \\mathbb{E}_{q(z_t\\mid z_{t-1})}\\left[\\log p(z_t\\mid z_{t-1})\\right ] \\quad\\text{by the linearity of } \\mathbb{E}[\\cdot]\\\\\n",
    "&=\\sum_{n=1}^N q(z_1 = n)\\log p(z_1 = n) + \\sum_{t=1}^T \\sum_{n=1}^N q( x_t \\mid z_t=n)\\log p(x_t \\mid z_t = n) \\\\ & \\quad\\quad + \\sum_{t=2}^T \\sum_{n=1}^N\\sum_{m=1}^M q(z_t = n \\mid z_{t-1} = m) \\log p(z_t = n\\mid z_{t-1} = m) \\\\\n",
    "&=\\sum_{n=1}^N q(z_1 = n)\\log\\pi_n + \\sum_{t=1}^T \\sum_{n=1}^N q( x_t \\mid z_t=n)\\log E_{n,x_t} + \\sum_{t=2}^T \\sum_{n=1}^N\\sum_{m=1}^N q(z_t = n \\mid z_{t-1} = m) \\log T_{n, m} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "However, we cannot maximise $L'(\\theta)$ just yet, as it does not incorporate the 3 probability assumptions. We therefore define a new function $L(\\theta)$ which does incorporate them using Lagrange multipliers:\n",
    "\n",
    "$$\n",
    "L(\\theta) = L'(\\theta) + \\alpha \\underbrace{\\left [ \\sum_{n = 1}^N \\pi_n - 1 \\right]}_{\\pi \\text{ is a pmf}} + \\beta \\underbrace{\\left [ \\sum_{n=1}^N \\sum_{k=1}^K  E_{n,k} - N \\right ]}_{\\text{each row $E$ of is a pmf}} + \\gamma \\underbrace{\\left [ \\sum_{n=1}^N \\sum_{m=1}^N  T_{n,m} - N \\right ]}_{\\text{each column $T$ of is a pmf}}\n",
    "$$\n",
    "\n",
    "This can now be differentiated w.r.t. $\\pi, E$ and $T$. \n",
    "\n",
    "First we take care of $\\pi$:\n",
    "\n",
    "$$\n",
    "\\diff{L(\\theta)}{\\pi_s} = \\frac{q(z_1 = s)}{\\pi_s} + \\alpha = 0 \\quad\\Rightarrow\\quad \\pi_s = -\\frac{q(z_1 = s)}{\\alpha}\n",
    "$$\n",
    "Then, since $\\sum_{n = 1}^N \\pi_n = 1$, we have\n",
    "\n",
    "$$\n",
    "\\sum_{n = 1}^N \\pi_s = -\\frac{1}{\\alpha}\\sum_{n = 1}^N q(z_1 = n) = 1 \\quad\\Rightarrow\\quad -\\sum_{n = 1}^N q(z_1 = n) = \\alpha\n",
    "$$\n",
    "\n",
    "Substituting this back, we get:\n",
    "$$\n",
    "    \\pi_s = \\frac{q(z_1 = s)}{\\sum_{n = 1}^N q(z_1 = n)}\n",
    "$$\n",
    "\n",
    "Next, we take care of $E$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\diff{L(\\theta)}{E_{n', k'}} &= \\beta + \\diff{}{E_{n', k'}}\\sum_{t=1}^T \\sum_{n=1}^N q( x_t \\mid z_t=n)\\log E_{n,x_t}\\\\\n",
    "& = \\beta + \\sum_{t=1}^T \\delta(x_t = k') \\frac{q( x_t \\mid z_t=n')}{E_{n',k'}} = 0 \\quad \\text{where $\\delta$ is the Kronecker delta}\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "Whence\n",
    "$$\n",
    "E_{n', k'} = -\\frac{1}{\\beta}\\sum_{t=1}^T \\delta(x_t = k') q( x_t \\mid z_t=n')\n",
    "$$\n",
    "\n",
    "Now, using the constraint that $ \\sum_{n=1}^N \\sum_{k=1}^K  E_{n,k} = N $:\n",
    "\n",
    "$$\n",
    "     \\sum_{n=1}^N \\sum_{k=1}^K  -\\frac{1}{\\beta}\\sum_{t=1}^T \\delta(x_t = k) q( x_t \\mid z_t=n) = N \\quad \\Rightarrow \\quad \\beta = -\\frac{1}{N}\\sum_{n=1}^N \\sum_{k=1}^K\\sum_{t=1}^T \\delta(x_t = k) q( x_t \\mid z_t=n)\n",
    "$$\n",
    "\n",
    "Therefore, we have:\n",
    "$$\n",
    "    E_{n, k} = \\frac{\\sum_{t=1}^T \\delta(x_t = k) q( x_t \\mid z_t=n)}{\\frac{1}{N}\\sum_{n=1}^N \\sum_{k=1}^K \\sum_{t=1}^T \\delta(x_t = k) q( x_t \\mid z_t=n)}\n",
    "$$\n",
    "\n",
    "Finally, we take care of $T$:\n",
    "\n",
    "$$\n",
    "    \\diff{L(\\theta)}{T_{n', m'}} = \\gamma + \\sum_{t=2}^T \\frac{q(z_t = n' \\mid z_{t-1} = m')}{T_{n', m'}} = 0 \\quad \\Rightarrow\\quad T_{n', m'} = - \\frac{1}{\\gamma}\\sum_{t=2}^T q(z_t = n' \\mid z_{t-1} = m')\n",
    "$$\n",
    "\n",
    "Using the constraint that $\\sum_{n=1}^N \\sum_{m=1}^N  T_{n,m} = N$:\n",
    "\n",
    "$$\n",
    "\\sum_{n=1}^N \\sum_{m=1}^N - \\frac{1}{\\gamma}\\sum_{t=2}^T q(z_t = n' \\mid z_{t-1} = m') = N \\quad \\Rightarrow\\quad \\gamma = - \\frac{1}{N}\\sum_{n=1}^N \\sum_{m=1}^N \\sum_{t=2}^T q(z_t = n' \\mid z_{t-1} = m')\n",
    "$$\n",
    "And hence:\n",
    "\n",
    "$$\n",
    "T_{n, m} = \\frac{\\sum_{t=2}^T q(z_t = n \\mid z_{t-1} = m)}{\\frac{1}{N}\\sum_{n=1}^N \\sum_{m=1}^N \\sum_{t=2}^T q(z_t = n \\mid z_{t-1} = m)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
